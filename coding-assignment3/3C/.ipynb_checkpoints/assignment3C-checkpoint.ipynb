{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3C: Denoising Diffusion Probabilistic Models (DDPM)\n",
    "\n",
    "In this assignment, you will implement a simplified version of the **Denoising Diffusion Probabilistic Model (DDPM)**, one of the foundational generative models behind modern AI image generators like Stable Diffusion and DALL-E.\n",
    "\n",
    "Diffusion models work by:\n",
    "1. **Forward Process**: Gradually adding Gaussian noise to data until it becomes pure noise\n",
    "2. **Reverse Process**: Learning to denoise, step by step, to generate new samples from random noise\n",
    "\n",
    "By the end of this lab, you will be able to generate handwritten digits from scratch using your own diffusion model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Below we define the hyperparameters for our diffusion model. Feel free to experiment with these values, except those marked as `DO NOT MODIFY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Training parameters\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 2e-4\n",
    "    num_epochs: int = 10\n",
    "    \n",
    "    # Diffusion parameters\n",
    "    num_timesteps: int = 1000  # Number of diffusion steps (T)\n",
    "    beta_start: float = 1e-4  # Starting noise level\n",
    "    beta_end: float = 0.02    # Ending noise level\n",
    "    \n",
    "    # Model parameters\n",
    "    image_size: int = 28      # MNIST image size (DO NOT MODIFY)\n",
    "    in_channels: int = 1      # MNIST is grayscale (DO NOT MODIFY)\n",
    "    hidden_dims: int = 64     # Hidden dimension for U-Net\n",
    "    \n",
    "    # Device\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    seed: int = 42\n",
    "\n",
    "config = Config()\n",
    "torch.manual_seed(config.seed)\n",
    "print(f\"Using device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We will use the MNIST dataset for this assignment. The images are normalized to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: normalize to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some samples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "sample_batch, _ = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(sample_batch[i].squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Noise Scheduler\n",
    "\n",
    "The noise scheduler defines how noise is added during the forward process. We use a linear schedule where $\\beta_t$ increases linearly from `beta_start` to `beta_end`.\n",
    "\n",
    "Key quantities:\n",
    "- $\\beta_t$: Noise variance at timestep $t$\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$ (cumulative product)\n",
    "\n",
    "The forward process is defined as:\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$\n",
    "\n",
    "This means we can sample $x_t$ directly from $x_0$:\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement the `NoiseScheduler` class\n",
    "\n",
    "Complete the `__init__` method to compute:\n",
    "1. `betas`: Linear schedule from `beta_start` to `beta_end`\n",
    "2. `alphas`: $\\alpha_t = 1 - \\beta_t$\n",
    "3. `alpha_cumprods`: $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
    "4. `sqrt_alpha_cumprods`: $\\sqrt{\\bar{\\alpha}_t}$\n",
    "5. `sqrt_one_minus_alpha_cumprods`: $\\sqrt{1 - \\bar{\\alpha}_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end, device):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # (1) Create linear schedule for betas\n",
    "        self.betas = \n",
    "        # (2) Compute alphas\n",
    "        self.alphas = \n",
    "        # (3) Compute cumulative product of alphas\n",
    "        self.alpha_cumprods = \n",
    "        # (4) Compute sqrt of alpha_cumprods\n",
    "        self.sqrt_alpha_cumprods = \n",
    "        # (5) Compute sqrt of (1 - alpha_cumprods)\n",
    "        self.sqrt_one_minus_alpha_cumprods = \n",
    "        \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Add noise to x_0 to get x_t using the reparameterization trick.\n",
    "        x_t = sqrt(alpha_cumprod_t) * x_0 + sqrt(1 - alpha_cumprod_t) * noise\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        sqrt_alpha_cumprod_t = self.sqrt_alpha_cumprods[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprods[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        x_t = sqrt_alpha_cumprod_t * x_0 + sqrt_one_minus_alpha_cumprod_t * noise\n",
    "        return x_t, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the noise scheduler\n",
    "scheduler = NoiseScheduler(\n",
    "    num_timesteps=config.num_timesteps,\n",
    "    beta_start=config.beta_start,\n",
    "    beta_end=config.beta_end,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the forward diffusion process to understand how noise is gradually added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the forward diffusion process\n",
    "sample_image = sample_batch[0:1].to(config.device)\n",
    "timesteps_to_show = [0, 100, 250, 500, 750, 999]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(15, 3))\n",
    "for i, t in enumerate(timesteps_to_show):\n",
    "    t_tensor = torch.tensor([t], device=config.device)\n",
    "    noisy_image, _ = scheduler.add_noise(sample_image, t_tensor)\n",
    "    axes[i].imshow(noisy_image[0].squeeze().cpu(), cmap='gray')\n",
    "    axes[i].set_title(f't = {t}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Forward Diffusion Process: Adding Noise Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Time Embedding\n",
    "\n",
    "The model needs to know the current timestep to predict the noise. We use sinusoidal positional embeddings (similar to Transformers) to encode the timestep.\n",
    "\n",
    "$$PE(t, 2i) = \\sin(t / 10000^{2i/d})$$\n",
    "$$PE(t, 2i+1) = \\cos(t / 10000^{2i/d})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement `SinusoidalPositionEmbedding`\n",
    "\n",
    "Complete the `forward` method to compute sinusoidal embeddings for the timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Create sinusoidal position embeddings for timestep t.\n",
    "        Args:\n",
    "            t: Tensor of shape (batch_size,) containing timesteps\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, dim) containing embeddings\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # (1) Compute the frequency divisor term\n",
    "        # (2) Compute position * frequency\n",
    "        # (3) Apply sin and cos, then concatenate\n",
    "        embeddings = \n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: U-Net Architecture\n",
    "\n",
    "The U-Net is the backbone of our diffusion model. It takes a noisy image and the timestep embedding, and predicts the noise that was added.\n",
    "\n",
    "The architecture consists of:\n",
    "1. **Encoder**: Downsamples the image while increasing channels\n",
    "2. **Middle Block**: Processes the compressed representation\n",
    "3. **Decoder**: Upsamples back to the original resolution with skip connections\n",
    "\n",
    "We'll implement a simplified version suitable for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with GroupNorm and SiLU activation.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "        # Time embedding projection\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "        \n",
    "        # Residual connection if channels don't match\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.act(self.norm1(self.conv1(x)))\n",
    "        \n",
    "        # Add time embedding\n",
    "        time_emb = self.act(self.time_mlp(t_emb))\n",
    "        h = h + time_emb[:, :, None, None]\n",
    "        \n",
    "        h = self.act(self.norm2(self.conv2(h)))\n",
    "        return h + self.residual_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement the U-Net `forward` method\n",
    "\n",
    "Complete the forward pass of the U-Net:\n",
    "1. Get time embeddings\n",
    "2. Encoder: Apply conv blocks and downsample, saving skip connections\n",
    "3. Middle: Apply middle block\n",
    "4. Decoder: Upsample, concatenate skip connections, and apply conv blocks\n",
    "5. Output: Project to the number of output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, hidden_dims=64, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            SinusoidalPositionEmbedding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.init_conv = nn.Conv2d(in_channels, hidden_dims, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Encoder (downsampling path)\n",
    "        self.down1 = ConvBlock(hidden_dims, hidden_dims, time_emb_dim)\n",
    "        self.down2 = ConvBlock(hidden_dims, hidden_dims * 2, time_emb_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Middle\n",
    "        self.middle = ConvBlock(hidden_dims * 2, hidden_dims * 2, time_emb_dim)\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.up1 = ConvBlock(hidden_dims * 4, hidden_dims, time_emb_dim)  # *4 because of skip connection\n",
    "        self.up2 = ConvBlock(hidden_dims * 2, hidden_dims, time_emb_dim)  # *2 because of skip connection\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Output\n",
    "        self.final_conv = nn.Conv2d(hidden_dims, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Forward pass of the U-Net.\n",
    "        Args:\n",
    "            x: Noisy image tensor of shape (batch_size, channels, height, width)\n",
    "            t: Timestep tensor of shape (batch_size,)\n",
    "        Returns:\n",
    "            Predicted noise tensor of same shape as x\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # (1) Get time embeddings\n",
    "        # (2) Initial convolution\n",
    "        # (3) Encoder path: down1 -> pool -> down2 -> pool (save skip connections)\n",
    "        # (4) Middle block\n",
    "        # (5) Decoder path: upsample -> concat skip2 -> up1 -> upsample -> concat skip1 -> up2\n",
    "        # (6) Final convolution\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleUNet(\n",
    "    in_channels=config.in_channels,\n",
    "    out_channels=config.in_channels,\n",
    "    hidden_dims=config.hidden_dims\n",
    ").to(config.device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training\n",
    "\n",
    "The training objective is simple: predict the noise that was added to the image.\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon$ is the actual noise added\n",
    "- $\\epsilon_\\theta(x_t, t)$ is the model's prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement the training loop\n",
    "\n",
    "Complete the training loop:\n",
    "1. Sample random timesteps\n",
    "2. Add noise to the images using the scheduler\n",
    "3. Predict the noise using the model\n",
    "4. Compute MSE loss between predicted and actual noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, scheduler, config):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, _ in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(config.device)\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # (1) Sample random timesteps\n",
    "        # (2) Add noise to images\n",
    "        # (3) Predict noise\n",
    "        # (4) Compute MSE loss\n",
    "        \n",
    "        t = \n",
    "        noisy_images, noise = \n",
    "        predicted_noise = \n",
    "        loss = \n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(config.num_epochs):\n",
    "    avg_loss = train_one_epoch(model, train_loader, optimizer, scheduler, config)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{config.num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Sampling (Image Generation)\n",
    "\n",
    "To generate images, we start from pure noise and iteratively denoise using our trained model.\n",
    "\n",
    "The sampling process (simplified DDPM):\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\\right) + \\sigma_t z$$\n",
    "\n",
    "Where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t = \\sqrt{\\beta_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement the `sample_step` function\n",
    "\n",
    "Complete the sampling step to denoise from $x_t$ to $x_{t-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_step(model, x_t, t, scheduler):\n",
    "    \"\"\"\n",
    "    Perform one denoising step: x_t -> x_{t-1}\n",
    "    \"\"\"\n",
    "    # Get the values we need from the scheduler\n",
    "    beta_t = scheduler.betas[t]\n",
    "    alpha_t = scheduler.alphas[t]\n",
    "    alpha_cumprod_t = scheduler.alpha_cumprods[t]\n",
    "    sqrt_one_minus_alpha_cumprod_t = scheduler.sqrt_one_minus_alpha_cumprods[t]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # (1) Predict the noise (need to create timestep tensor for the model)\n",
    "    # (2) Compute the noise coefficient\n",
    "    # (3) Compute the mean of x_{t-1}\n",
    "    # (4) Add noise for t > 0, otherwise just return mean\n",
    "    \n",
    "    x_t_minus_1 = \n",
    "    \n",
    "    return x_t_minus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, scheduler, config, num_samples=16):\n",
    "    \"\"\"\n",
    "    Generate samples by iteratively denoising from pure noise.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(num_samples, config.in_channels, config.image_size, config.image_size).to(config.device)\n",
    "    \n",
    "    # Iteratively denoise\n",
    "    for t in tqdm(range(config.num_timesteps - 1, -1, -1), desc=\"Sampling\"):\n",
    "        x = sample_step(model, x, t, scheduler)\n",
    "    \n",
    "    # Clamp to [-1, 1] and convert to [0, 1] for visualization\n",
    "    x = torch.clamp(x, -1, 1)\n",
    "    x = (x + 1) / 2\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "generated_samples = sample(model, scheduler, config, num_samples=16)\n",
    "\n",
    "# Visualize generated samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated_samples[i].squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Generated MNIST Digits')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualize the Reverse Diffusion Process\n",
    "\n",
    "Let's visualize how the model gradually denoises to generate an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_with_trajectory(model, scheduler, config):\n",
    "    \"\"\"Sample and save intermediate steps for visualization.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.randn(1, config.in_channels, config.image_size, config.image_size).to(config.device)\n",
    "    trajectory = [x.clone()]\n",
    "    \n",
    "    save_steps = [999, 900, 750, 500, 250, 100, 50, 0]\n",
    "    \n",
    "    for t in range(config.num_timesteps - 1, -1, -1):\n",
    "        x = sample_step(model, x, t, scheduler)\n",
    "        if t in save_steps:\n",
    "            trajectory.append(x.clone())\n",
    "    \n",
    "    return trajectory, save_steps\n",
    "\n",
    "trajectory, steps = sample_with_trajectory(model, scheduler, config)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(trajectory), figsize=(15, 2))\n",
    "titles = ['t=1000\\n(noise)'] + [f't={s}' for s in steps[:-1]] + ['t=0\\n(generated)']\n",
    "for i, (img, title) in enumerate(zip(trajectory, titles)):\n",
    "    img = torch.clamp(img, -1, 1)\n",
    "    img = (img + 1) / 2\n",
    "    axes[i].imshow(img[0].squeeze().cpu(), cmap='gray')\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Reverse Diffusion Process: Denoising Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config\n",
    "}, 'diffusion_model.pt')\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this assignment, you implemented:\n",
    "\n",
    "1. **Noise Scheduler**: Defines how noise is added during the forward process\n",
    "2. **Sinusoidal Position Embedding**: Encodes the timestep for the model\n",
    "3. **U-Net Architecture**: Predicts the noise added to images\n",
    "4. **Training Loop**: Optimizes the model to predict noise\n",
    "5. **Sampling**: Generates new images by iteratively denoising\n",
    "\n",
    "Congratulations! You've built a working diffusion model from scratch. This is the same fundamental approach used in state-of-the-art image generation models like Stable Diffusion, DALL-E, and Imagen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
