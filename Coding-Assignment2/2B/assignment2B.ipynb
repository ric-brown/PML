{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Assignment 2 - Part B: k-Nearest Neighbor Classification\n",
        "Please refer to the `README.pdf` for full laboratory instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement\n",
        "In this part, you will implement the k-Nearest Neighbor (k-NN) classifier and evaluate it on two datasets:\n",
        "- **Lenses Dataset**: A small dataset for contact lens prescription\n",
        "- **Credit Approval (CA) Dataset**: Credit card application data with binary labels (+/-)\n",
        "\n",
        "### Your Tasks\n",
        "1. **Preprocess the data**: Handle missing values and normalize features\n",
        "2. **Implement k-NN** with L2 distance\n",
        "3. **Evaluate** on both datasets for different values of k\n",
        "4. **Discuss** your results\n",
        "\n",
        "### Datasets\n",
        "The data files are located in the `credit 2017/` folder:\n",
        "- `lenses.training`, `lenses.testing`\n",
        "- `crx.data.training`, `crx.data.testing`\n",
        "- `crx.names` (describes the features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library declarations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenses - Train: (18, 3), Test: (6, 3)\n"
          ]
        }
      ],
      "source": [
        "# Data paths\n",
        "DATA_PATH = \"credit_2017/\"\n",
        "\n",
        "# Load Lenses data\n",
        "def load_lenses_data():\n",
        "    \"\"\"Load the lenses dataset.\"\"\"\n",
        "    train_data = np.loadtxt(DATA_PATH + \"lenses.training\", delimiter=',')\n",
        "    test_data = np.loadtxt(DATA_PATH + \"lenses.testing\", delimiter=',')\n",
        "    \n",
        "    # First column is ID, last column is label\n",
        "    X_train = train_data[:, 1:-1]\n",
        "    y_train = train_data[:, -1]\n",
        "    X_test = test_data[:, 1:-1]\n",
        "    y_test = test_data[:, -1]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load Credit Approval data\n",
        "def load_credit_data(train_file, test_file):\n",
        "    \"\"\"\n",
        "    Load the Credit Approval dataset.\n",
        "    Note: This dataset contains missing values (?) and mixed types.\n",
        "    You will need to preprocess it.\n",
        "    \"\"\"\n",
        "    def _read(path):\n",
        "        rows=[]\n",
        "        labels=[]\n",
        "        with open(path,'r') as f:\n",
        "            for ln in f:\n",
        "                ln=ln.strip()\n",
        "                if not ln:\n",
        "                    continue\n",
        "                parts=[p.strip() for p in ln.split(',')]\n",
        "                rows.append(parts[:-1])\n",
        "                labels.append(parts[-1])\n",
        "        return rows, labels\n",
        "    train_rows, train_labels = _read(train_file)\n",
        "    test_rows, test_labels = _read(test_file)\n",
        "    return train_rows, train_labels, test_rows, test_labels\n",
        "    \n",
        "\n",
        "# Test loading lenses data\n",
        "X_train_lenses, y_train_lenses, X_test_lenses, y_test_lenses = load_lenses_data()\n",
        "print(f\"Lenses - Train: {X_train_lenses.shape}, Test: {X_test_lenses.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data Preprocessing\n",
        "For the Credit Approval dataset, you need to:\n",
        "1. **Handle missing values** (marked with '?'):\n",
        "   - Categorical features: replace with mode/median\n",
        "   - Numerical features: replace with label-conditioned mean\n",
        "2. **Normalize features** using z-scaling:\n",
        "   $$z_i^{(m)} = \\frac{x_i^{(m)} - \\mu_i}{\\sigma_i}$$\n",
        "\n",
        "Document exactly how you handle each feature!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess_credit_data(train_file, test_file):\n",
        "    \"\"\"\n",
        "    Preprocess the Credit Approval dataset.\n",
        "\n",
        "    Steps:\n",
        "    1) Load the data\n",
        "    2) Handle missing values\n",
        "       - Categorical: replace '?' with MODE of that column computed on TRAIN only\n",
        "       - Numerical:   replace '?' with label-conditioned MEAN computed on TRAIN only\n",
        "                      (mean of that feature among samples with the same label)\n",
        "    3) Encode categorical variables (fit mappings on TRAIN only)\n",
        "    4) Normalize numerical features using z-score (TRAIN mean/std), applied to train + test\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, y_train, X_test, y_test : numpy arrays\n",
        "    \"\"\"\n",
        "\n",
        "    # A1-A15 are features, A16 is label\n",
        "    # Numerical (continuous): A2, A3, A8, A11, A14, A15 -> 0-based indices: 1,2,7,10,13,14\n",
        "    num_idx = [1, 2, 7, 10, 13, 14]\n",
        "    cat_idx = [i for i in range(15) if i not in num_idx]\n",
        "\n",
        "    # (Index -> Name -> Type)\n",
        "    feature_docs = [\n",
        "        (0,  \"A1\",  \"categorical\"),\n",
        "        (1,  \"A2\",  \"numerical\"),\n",
        "        (2,  \"A3\",  \"numerical\"),\n",
        "        (3,  \"A4\",  \"categorical\"),\n",
        "        (4,  \"A5\",  \"categorical\"),\n",
        "        (5,  \"A6\",  \"categorical\"),\n",
        "        (6,  \"A7\",  \"categorical\"),\n",
        "        (7,  \"A8\",  \"numerical\"),\n",
        "        (8,  \"A9\",  \"categorical\"),\n",
        "        (9,  \"A10\", \"categorical\"),\n",
        "        (10, \"A11\", \"numerical\"),\n",
        "        (11, \"A12\", \"categorical\"),\n",
        "        (12, \"A13\", \"categorical\"),\n",
        "        (13, \"A14\", \"numerical\"),\n",
        "        (14, \"A15\", \"numerical\"),\n",
        "    ]\n",
        "\n",
        "    # 1) Load raw data \n",
        "    train_rows, train_labels, test_rows, test_labels = load_credit_data(train_file, test_file)\n",
        "\n",
        "    # Label encoding: '+' -> 1, '-' -> 0\n",
        "    y_train = np.array([1 if l.strip() == '+' else 0 for l in train_labels], dtype=int)\n",
        "    y_test  = np.array([1 if l.strip() == '+' else 0 for l in test_labels], dtype=int)\n",
        "\n",
        "    # 2) TRAIN only imputation stats\n",
        "    cat_mode = {}\n",
        "    for j in cat_idx:\n",
        "        counts = {}\n",
        "        for r in train_rows:\n",
        "            v = r[j]\n",
        "            if v != '?':\n",
        "                counts[v] = counts.get(v, 0) + 1\n",
        "        cat_mode[j] = max(counts, key=counts.get)\n",
        "\n",
        "    num_mean = {}\n",
        "    for j in num_idx:\n",
        "        for c in [0, 1]:\n",
        "            vals = []\n",
        "            for r, y in zip(train_rows, y_train):\n",
        "                if y == c and r[j] != '?':\n",
        "                    vals.append(float(r[j]))\n",
        "            if len(vals) == 0:\n",
        "                vals = [float(r[j]) for r in train_rows if r[j] != '?']\n",
        "            num_mean[(j, c)] = float(np.mean(vals))\n",
        "\n",
        "    # 3) Impute missing values\n",
        "    def impute(rows, y_vec):\n",
        "        out = []\n",
        "        for r, c in zip(rows, y_vec):\n",
        "            r2 = list(r)\n",
        "            for j in cat_idx:\n",
        "                if r2[j] == '?':\n",
        "                    r2[j] = cat_mode[j]\n",
        "            for j in num_idx:\n",
        "                if r2[j] == '?':\n",
        "                    r2[j] = str(num_mean[(j, int(c))])\n",
        "            out.append(r2)\n",
        "        return out\n",
        "\n",
        "    train_imp = impute(train_rows, y_train)\n",
        "    test_imp  = impute(test_rows, y_test)\n",
        "\n",
        "    # 4) Encode categoricals \n",
        "    cat_maps = {}\n",
        "    for j in cat_idx:\n",
        "        uniq = sorted({r[j] for r in train_imp})\n",
        "        cat_maps[j] = {val: idx for idx, val in enumerate(uniq)}\n",
        "\n",
        "    def encode(rows):\n",
        "        X = np.zeros((len(rows), 15), dtype=float)\n",
        "        for i, r in enumerate(rows):\n",
        "            for j in range(15):\n",
        "                if j in num_idx:\n",
        "                    X[i, j] = float(r[j])\n",
        "                else:\n",
        "                    X[i, j] = float(cat_maps[j][r[j]])\n",
        "        return X\n",
        "\n",
        "    X_train = encode(train_imp)\n",
        "    X_test  = encode(test_imp)\n",
        "\n",
        "    # 5) Z-normalize numeric columns\n",
        "    X_train, X_test = z_normalize(X_train, X_test, num_idx)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def z_normalize(X_train, X_test, feature_indices):\n",
        "    X_train = X_train.astype(float).copy()\n",
        "    X_test  = X_test.astype(float).copy()\n",
        "    for j in feature_indices:\n",
        "        mu = float(np.mean(X_train[:, j]))\n",
        "        sigma = float(np.std(X_train[:, j]))\n",
        "        if sigma == 0.0:\n",
        "            sigma = 1.0\n",
        "        X_train[:, j] = (X_train[:, j] - mu) / sigma\n",
        "        X_test[:, j]  = (X_test[:, j] - mu) / sigma\n",
        "    return X_train, X_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Implement k-NN Classifier\n",
        "Implement k-NN with L2 (Euclidean) distance:\n",
        "$$\\mathcal{D}_{L2}(\\mathbf{a}, \\mathbf{b}) = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n",
        "\n",
        "For **categorical attributes**, use:\n",
        "- Distance = 1 if values are different\n",
        "- Distance = 0 if values are the same\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l2_distance(a, b, num_idx=None, cat_idx=None):\n",
        "    \"\"\"\n",
        "    Compute L2 (Euclidean) distance between two vectors.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    a, b : numpy arrays of same shape\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    distance : float\n",
        "    \"\"\"\n",
        "    a = np.asarray(a)\n",
        "    b = np.asarray(b)\n",
        "    total = 0.0\n",
        "\n",
        "    if num_idx is None:\n",
        "        # default to Euclidean over all dims\n",
        "        diff = a - b\n",
        "        return float(np.sqrt(np.sum(diff * diff)))\n",
        "\n",
        "    for j in num_idx:\n",
        "        d = float(a[j] - b[j])\n",
        "        total += d * d\n",
        "    for j in cat_idx:\n",
        "        total += 0.0 if int(a[j]) == int(b[j]) else 1.0\n",
        "    return float(np.sqrt(total))\n",
        "\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k, num_idx=None, cat_idx=None):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using k-NN.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy array of shape (n_train, n_features)\n",
        "    y_train : numpy array of shape (n_train,)\n",
        "    X_test : numpy array of shape (n_test, n_features)\n",
        "    k : int, number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy array of shape (n_test,)\n",
        "    \"\"\"\n",
        "    X_train = np.asarray(X_train)\n",
        "    y_train = np.asarray(y_train).astype(int)\n",
        "    X_test = np.asarray(X_test)\n",
        "\n",
        "    preds = np.zeros((X_test.shape[0],), dtype=int)\n",
        "\n",
        "    for i, x in enumerate(X_test):\n",
        "        # compute distances\n",
        "        dists = np.zeros((X_train.shape[0],), dtype=float)\n",
        "        for j, xt in enumerate(X_train):\n",
        "            dists[j] = l2_distance(x, xt, num_idx=num_idx, cat_idx=cat_idx)\n",
        "\n",
        "        nn_idx = np.argsort(dists)[:k]\n",
        "        nn_labels = y_train[nn_idx]\n",
        "\n",
        "        # nearest neighbor vote\n",
        "        preds[i] = np.bincount(nn_labels).argmax()        \n",
        "        nn_idx = np.argsort(dists)[:k]\n",
        "        nn_labels = y_train[nn_idx]\n",
        "        preds[i] = np.bincount(nn_labels).argmax()\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    accuracy : float (between 0 and 1)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    return float(np.mean(y_true == y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Evaluate on Lenses Dataset\n",
        "Test your k-NN implementation on the Lenses dataset for different values of k.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 1.0000\n",
            "k=3: Accuracy = 1.0000\n",
            "k=5: Accuracy = 0.5000\n",
            "k=7: Accuracy = 0.8333\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Lenses dataset\n",
        "# Try different values of k (e.g., 1, 3, 5, 7)\n",
        "\n",
        "k_values = [1, 3, 5, 7]\n",
        "lenses_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_lenses, y_train_lenses, X_test_lenses, k)\n",
        "    accuracy = compute_accuracy(y_test_lenses, predictions)\n",
        "    lenses_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Evaluate on Credit Approval Dataset\n",
        "First preprocess the data, then evaluate k-NN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Credit Approval - Train: (552, 15), Test: (138, 15)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Preprocess Credit Approval data\n",
        "X_train_credit, y_train_credit, X_test_credit, y_test_credit = preprocess_credit_data(\n",
        "    DATA_PATH + \"crx.data.training\",\n",
        "    DATA_PATH + \"crx.data.testing\"\n",
        ")\n",
        "\n",
        "print(f\"Credit Approval - Train: {X_train_credit.shape}, Test: {X_test_credit.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 0.7464\n",
            "k=3: Accuracy = 0.7826\n",
            "k=5: Accuracy = 0.8043\n",
            "k=7: Accuracy = 0.7971\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Credit Approval dataset\n",
        "k_values = [1, 3, 5, 7]\n",
        "credit_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
        "    accuracy = compute_accuracy(y_test_credit, predictions)\n",
        "    credit_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "### Results Table\n",
        "\n",
        "| Dataset | k=1 | k=3 | k=5 | k=7 |\n",
        "|---------|-----|-----|-----|-----|\n",
        "| Lenses | 100.00 | 100.00 | 50.00 | 83.33 |\n",
        "| Credit Approval | 74.64 | 78.26 | 80.43 | 79.71 |\n",
        "\n",
        "### Discussion\n",
        "*Answer these questions:*\n",
        "1. Which value of k works best for each dataset? Why do you think that is?\n",
        "    -  For the Lenses dataset a lower k value works best. This is because the dataset is small and limited meaning edge cases will be misclasified. \n",
        "    - For the credit score, higher k values yield better results. This is due to the dataset being much larger, and having more features. This allows the data to seperate from classes and allow machine learning methods to understand the underlying patterns between features.\n",
        "2. How did preprocessing affect your results on the Credit Approval dataset?\n",
        "    - Preproccessing is essential in eliminating bias and noise. Most importantly scaling the data allows for an \"even playing field\" between data points. Larger values are not overshadowing the smaller more subtle patterns. \n",
        "3. What are the trade-offs of using different values of k?\n",
        "    - When you have a limited dataset and a high number of K nieghbors, the edge cases tend to get misclassified. It applies to larger datasets as well, however, the effects are mitigated because there are more datapoints to reflect the real world. The best course of action would be to preproccess the dataset, then begin training the model. A method such as Bayesian Optimization can be employed to automatically tune a model to the best k nearest neighbors.\n",
        "4. What did you learn from this exercise?\n",
        "    -   This exercise was very interesting. Preprocessing is the most essential stage in machine leanring. If not implemented correctly the ML model can crash. It was very cool implementing the statistical methods from scratch such as knn with l2 distance\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
